{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from torchvision import transforms\n",
    "from scipy.io.wavfile import read\n",
    "import IPython.display as ipd\n",
    "from text import cleaners\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--filter_length', default=1024, type=int, help= 'filter_length')\n",
    "parser.add_argument('--segment_size', default=8192, type=int, help= 'segment_size')\n",
    "parser.add_argument('--hop_length', default=256, type=int, help= 'hop_length')\n",
    "parser.add_argument('--inter_channels', default=192, type=int, help= 'inter_channels')\n",
    "parser.add_argument('--hidden_channels', default=192, type=int, help= 'hidden_channels')\n",
    "parser.add_argument('--filter_channels', default=768, type=int, help= 'filter_channels')\n",
    "parser.add_argument('--n_heads', default=2, type=int, help= 'n_heads')\n",
    "parser.add_argument('--n_layers', default=6, type=int, help= 'n_layers')\n",
    "parser.add_argument('--kernel_size', default=3, type=int, help= 'kernel_size')\n",
    "parser.add_argument('--p_dropout', default=0.1, type=float, help= 'p_dropout')\n",
    "parser.add_argument('--resblock', default=\"1\", type=str, help= 'resblock')\n",
    "parser.add_argument('--resblock_kernel_sizes', default=[3,7,11], type=list, help= 'resblock_kernel_sizes')\n",
    "parser.add_argument('--resblock_dilation_sizes', default=[[1,3,5], [1,3,5], [1,3,5]], type=list, help= 'resblock_dilation_sizes')\n",
    "parser.add_argument('--upsample_rates', default=[8,8,2,2], type=list, help= 'upsample_rates')\n",
    "parser.add_argument('--upsample_initial_channel', default=512, type=int, help= 'upsample_initial_channel')\n",
    "parser.add_argument('--upsample_kernel_sizes', default=[16,16,4,4], type=list, help= 'upsample_kernel_sizes')\n",
    "parser.add_argument('--gin_channels', default=128, type=int, help= 'gin_channels')\n",
    "parser.add_argument('--model_name', default='FVTTS', help= 'model_name')\n",
    "parser.add_argument('--GPU', default=0, type=str, help= 'GPU')\n",
    "\n",
    "args = parser.parse_args(args= [])\n",
    "args.model_dir = f'{args.save_path}/{args.model_name}'\n",
    "device = torch.device(f'cuda:{args.GPU}' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pad        = '_'\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
    "  assert os.path.isfile(checkpoint_path)\n",
    "  checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "  iteration = checkpoint_dict['iteration']\n",
    "  learning_rate = checkpoint_dict['learning_rate']\n",
    "  print(f'ITER {iteration} - LR : {learning_rate}')\n",
    "      \n",
    "  saved_state_dict = checkpoint_dict['model']\n",
    "  if hasattr(model, 'module'):\n",
    "      state_dict = model.state_dict()\n",
    "  new_state_dict= {}\n",
    "  for k, v in state_dict.items():\n",
    "      try:\n",
    "          new_state_dict[k] = saved_state_dict[k]\n",
    "      except:\n",
    "          print(\"%s is not in the checkpoint\" % k)\n",
    "          new_state_dict[k] = v\n",
    "  model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_model\n",
    "import torch.nn as nn\n",
    "net_g_face = face_model.SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    args.filter_length // 2 + 1,\n",
    "    args.segment_size // args.hop_length, \n",
    "    args.inter_channels, \n",
    "    args.hidden_channels, \n",
    "    args.filter_channels, \n",
    "    args.n_heads, \n",
    "    args.n_layers, \n",
    "    args.kernel_size, \n",
    "    args.p_dropout, \n",
    "    args.resblock, \n",
    "    args.resblock_kernel_sizes, \n",
    "    args.resblock_dilation_sizes, \n",
    "    args.upsample_rates, \n",
    "    args.upsample_initial_channel, \n",
    "    args.upsample_kernel_sizes, \n",
    "    \n",
    "    args.gin_channels).to(device)\n",
    "\n",
    "net_g_face = nn.DataParallel(net_g_face)\n",
    "c_path = f'{args.model_dir}/G_bestDloss.pth'\n",
    "net_g_face.eval()\n",
    "load_checkpoint(f'{c_path}', net_g_face, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def _clean_text(text, cleaner_names):\n",
    "  for name in cleaner_names:\n",
    "    cleaner = getattr(cleaners, name)\n",
    "    if not cleaner:\n",
    "      raise Exception('Unknown cleaner: %s' % name)\n",
    "    text = cleaner(text)\n",
    "  return text\n",
    "def cleaned_text_to_sequence(cleaned_text):\n",
    "  sequence = [_symbol_to_id[symbol] for symbol in cleaned_text]\n",
    "  return sequence\n",
    "\n",
    "def intersperse(lst, item):\n",
    "  result = [item] * (len(lst) * 2 + 1)\n",
    "  result[1::2] = lst\n",
    "  return result\n",
    "\n",
    "img_tf = train_transform()\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_path = 'image/'\n",
    "text = 'AND HE WAS TALKING ABOUT THE IMPORTANCE OF COACHING BOYS INTO MEN AND CHANGING THE CULTURE OF THE LOCKER ROOM AND GIVING'\n",
    "save_path = 'voice.wav'\n",
    "\n",
    "img = Image.open(i_path)\n",
    "image_emb = img_tf(img)\n",
    "content = resnet(image_emb.unsqueeze(0)).T\n",
    "\n",
    "clean_text = _clean_text(text, [\"english_cleaners2\"])\n",
    "text_norm = cleaned_text_to_sequence(clean_text)\n",
    "text_norm = intersperse(text_norm, 0)\n",
    "text_norm = torch.LongTensor(text_norm)\n",
    "x_tst_ = text_norm.unsqueeze(0).to(device)\n",
    "x_tst_lengths_ = torch.LongTensor([text_norm.size(0)]).to(device)\n",
    "\n",
    "face_audio = net_g_face.module.infer(x_tst_.to(device), x_tst_lengths_.to(device), img=image_emb.to(device), content = content.to(device), noise_scale=0.667, noise_scale_w=0.8, length_scale=1.2)[0][0,0].data.cpu().float().numpy()\n",
    "    \n",
    "with open(save_path,'wb') as f:\n",
    "    f.write(ipd.Audio(face_audio*2, rate=args.sampling_rate, normalize=False).data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
